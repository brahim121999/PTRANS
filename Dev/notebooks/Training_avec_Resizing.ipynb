{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuNJWHqZV3t"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Notebook pour le training de modèle pour le PTRANS**\n",
        "\n",
        "☕*Created by :*\n",
        "\n",
        "*   Mattéo Boursault\n",
        "*   Ibrahim Braham\n",
        "*   Adam Creusevault\n",
        "\n",
        "Dernière modification : *25/03/2022*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pour laisser tourner sur une longue durée**\n",
        "\n",
        "```\n",
        "var startClickConnect = function startClickConnect(){\n",
        "    var clickConnect = function clickConnect(){\n",
        "        console.log(\"Connnect Clicked - Start\");\n",
        "        document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "        console.log(\"Connnect Clicked - End\"); \n",
        "    };\n",
        "\n",
        "    var intervalId = setInterval(clickConnect, 180000);\n",
        "\n",
        "    var stopClickConnectHandler = function stopClickConnect() {\n",
        "        console.log(\"Connnect Clicked Stopped - Start\");\n",
        "        clearInterval(intervalId);\n",
        "        console.log(\"Connnect Clicked Stopped - End\");\n",
        "    };\n",
        "```"
      ],
      "metadata": {
        "id": "sSt3xro_3qng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faOh2PruarEc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "- Complétez le chemin des 'Verite_terrain'\n",
        "- Modifiez les paramètres dans la section Configuration au besoin\n",
        "- Executez toutes les cellules du Notebook\n",
        "- Have fun\n",
        "'''\n",
        "VERITE_PATH = 'drive/MyDrive/PTRANS/ptrans-main/Dev/data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zb4ECnTijSN"
      },
      "source": [
        "#### Import + Connexion au Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld2XJNfdefn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d711dcf7-f169-4a8f-f1ca-da76fd07fb92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCQ9g--jgH-5"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!CUDA_LAUNCH_BLOCKING=1 # plus utile de croiser les doigts mais bon "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PELvrwSvempV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import copy\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irBzptlBK18c"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY5OHw3RgIva"
      },
      "outputs": [],
      "source": [
        "class RadioDataset(Dataset):\n",
        "    def __init__(self, inputs_chiens, inputs_chats, transform=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # Chemins des dossiers de la vérité terrain\n",
        "        ROOT = VERITE_PATH + 'Verite_terrain/'\n",
        "        DOG_PATH = 'Chiens/'\n",
        "        CAT_PATH = 'Chats/'\n",
        "        RADIOS = 'Radios/'\n",
        "        HEART_MASKS = 'Coeur/'\n",
        "        VTB_MASKS = 'Vertebres/'\n",
        "        PROCESS_MASKS = 'Process_epineux/'\n",
        "\n",
        "        # Récupération et stockage des chemins des fichiers\n",
        "        dog_radios = self.generate_chemin(ROOT + DOG_PATH + RADIOS, inputs_chiens)\n",
        "        cat_radios = self.generate_chemin(ROOT + CAT_PATH + RADIOS, inputs_chats)\n",
        "\n",
        "        dog_hearts = self.generate_chemin(ROOT + DOG_PATH + HEART_MASKS, inputs_chiens, True)\n",
        "        cat_hearts = self.generate_chemin(ROOT + CAT_PATH + HEART_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_vtb = self.generate_chemin(ROOT + DOG_PATH + VTB_MASKS, inputs_chiens, True)\n",
        "        cat_vtb = self.generate_chemin(ROOT + CAT_PATH + VTB_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_prc = self.generate_chemin(ROOT + DOG_PATH + PROCESS_MASKS, inputs_chiens, True)\n",
        "        cat_prc = self.generate_chemin(ROOT + CAT_PATH + PROCESS_MASKS, inputs_chats, True)\n",
        "\n",
        "        self.radios_arr = dog_radios + cat_radios\n",
        "        self.heart_arr = dog_hearts + cat_hearts\n",
        "        self.vtb_arr = dog_vtb + cat_vtb\n",
        "        self.process_arr = dog_prc + cat_prc\n",
        "\n",
        "        self.data_len = len(self.radios_arr)\n",
        "\n",
        "        print(\"GROUND_TRUTH FOUND : \", self.data_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fonction obligatoire utilisée par le loader lors de l'itération du dataset\n",
        "        Récupération du chemin et chargement de la vérité terrain puis labellisation\n",
        "        \"\"\"\n",
        "\n",
        "        radio = np.array(Image.open(self.radios_arr[idx]))\n",
        "\n",
        "        heart_mask = self.grayscale(self.heart_arr[idx])\n",
        "        vtb_mask = self.grayscale(self.vtb_arr[idx])\n",
        "        process_mask = self.grayscale(self.process_arr[idx])\n",
        "\n",
        "        heart_mask[heart_mask != 0] = 1\n",
        "        vtb_mask[vtb_mask != 0] = 2\n",
        "        process_mask[process_mask != 0] = 3\n",
        "        #masks = heart_mask + vtb_mask + process_mask\n",
        "\n",
        "        \n",
        "        masks = np.zeros_like(heart_mask)\n",
        "        masks[heart_mask == 1] = 1\n",
        "        masks[vtb_mask == 2] = 2\n",
        "        masks[process_mask == 3] = 3\n",
        "        \n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=radio,mask=masks)\n",
        "            radio = augmentations['image']\n",
        "            masks = augmentations['mask']\n",
        "\n",
        "        grayscale_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                                  transforms.ToTensor()])\n",
        "        radio = Image.fromarray(radio)\n",
        "        radio = grayscale_transform(radio)\n",
        "\n",
        "        \"\"\"\n",
        "        background = np.zeros_like(masks) \n",
        "        heart_mask = np.zeros_like(masks)\n",
        "        vtb_mask = np.zeros_like(masks)\n",
        "        process_mask = np.zeros_like(masks)\n",
        "\n",
        "        background[masks == 0] = 1\n",
        "        heart_mask[masks == 1] = 1\n",
        "        vtb_mask[masks == 2] = 1\n",
        "        process_mask[masks == 3] = 1\n",
        "        \n",
        "        #masks = torch.tensor([heart_mask, vtb_mask, process_mask, background])\n",
        "        masks = np.array([heart_mask, vtb_mask, process_mask, background])\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Utiliser from_numpy plutôt que toTensor permet de ne pas normaliser les\n",
        "        données afin de respecter la labellisation des classes par 0, 1, 2 et 3\n",
        "        \"\"\"\n",
        "        return {\"image\": radio, \"mask\": torch.from_numpy(masks).to(DEVICE)}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Méthode nécessaire au loader qui retourne le\n",
        "        nombre de données dans le dataset\n",
        "        \"\"\"\n",
        "        return self.data_len\n",
        "\n",
        "\n",
        "    def grayscale(self, path):\n",
        "        image = Image.open(path)\n",
        "        image.load()\n",
        "        background = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
        "        background.paste(image, mask=image.split()[3])\n",
        "        image = background\n",
        "        image = image.convert('L')\n",
        "        return np.array(image)\n",
        "\n",
        "    def generate_chemin(self, path, tableau, jpgTopng=False):\n",
        "        tab = []\n",
        "        if (jpgTopng):\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i][0:-3] + 'png')\n",
        "        else:\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i])\n",
        "        return tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF_zEzHK6uS"
      },
      "source": [
        "#### Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI1mPoWg0p9I"
      },
      "outputs": [],
      "source": [
        "# modele UNet :\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Transposed convolution then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels , out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward_padding(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = nn.functionnal.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "    def forward(self, target_tensor, contracting_tensor):\n",
        "        target_tensor = self.up(target_tensor)\n",
        "        target_height = target_tensor.size()[2]\n",
        "        target_width = target_tensor.size()[3]\n",
        "        crop = transforms.CenterCrop((target_height, target_width))\n",
        "\n",
        "        contracting_tensor = crop(contracting_tensor)\n",
        "        new_tensor = torch.cat((target_tensor, contracting_tensor), 1)\n",
        "\n",
        "        return self.conv(new_tensor)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjPW8B2LAyh"
      },
      "source": [
        "#### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oDHw-0Fbhu"
      },
      "outputs": [],
      "source": [
        "MODELE = UNet                 # modèle utilisé, voir dans la partie Modèle du notebook la liste des modèles disponibles\n",
        "NB_CLASSES = 4                # 4 pour : coeur, vertebres, process_epineux, fond\n",
        "NB_CHANNELS = 1               # un vecteur en sortie (la prédiction du modèle)\n",
        "LEARNING_RATE = 0.001         # le taux d'apprentissage\n",
        "TRAIN_SIZE=0.9                # le % de segmentations allouées au jeu d'entrainement (1-TRAIN_SIZE pour la taille du jeu de validation)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # pour utiliser CUDA si nécessaire, ne pas toucher\n",
        "BATCH_SIZE = 1                # la taille de votre batch\n",
        "EPOCHS = 80                   # le nombre de fois ou vous souhaitez itérer sur l'ensemble du jeu de donnée pour entrainer votre modèle\n",
        "IMAGE_HEIGHT = 560            # la taille souhaité des radio en entrée du modèle (max 572 sans colab pro, 1520 avec), doit être un multiple de 16 (prérequis du modèle UNet)\n",
        "IMAGE_WIDTH = IMAGE_HEIGHT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bECYijM3b-"
      },
      "source": [
        "#### Data Augmentations (with Albumentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSJz3AgW9UWf"
      },
      "outputs": [],
      "source": [
        "# les transformations à appliquer à partir de la bibliothèque Albumentations qui vont servir à resize les radios et les masques en dimension unique 572 x 572\n",
        "# la réduction des dimensions va permettre que le training se déroule sans plantage de la GPU dû à un débordement de mémoire  \n",
        "# et aussi permettre la data augmentation en appliquant un nombre important d'epochs car les transformations sont aléatoires.\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Rotate(limit=20,p=0.4),\n",
        "    A.HorizontalFlip(p=0.4),\n",
        "    A.VerticalFlip(p=0.4),\n",
        "    A.Transpose(p=0.4),\n",
        "    A.GridDistortion(p=0.2),\n",
        "    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH)\n",
        "], additional_targets={'mask': 'image'})\n",
        "\n",
        "validation_transform = A.Compose([\n",
        "    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n",
        "], additional_targets={'mask': 'image'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etGkxf6aY-nf"
      },
      "source": [
        "#### Création des Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAPLPp8WNue4",
        "outputId": "f0e67c75-7203-4da4-a64d-352fcaaaaf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GROUND_TRUTH FOUND :  230\n",
            "GROUND_TRUTH FOUND :  27\n"
          ]
        }
      ],
      "source": [
        "# on a éclaté les data (radios et masques) en data train (90%) et data validation (10%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inputs_chiens = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chiens/Radios')\n",
        "inputs_chats = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chats/Radios')\n",
        "\n",
        "chiens_train, chiens_test = train_test_split(inputs_chiens, train_size=TRAIN_SIZE)\n",
        "chats_train, chats_test = train_test_split(inputs_chats, train_size=TRAIN_SIZE)\n",
        "\n",
        "# création des datasets pour le train et pour la validation\n",
        "# création des dataloaders pour le train et pour la validation\n",
        "\n",
        "train_data = RadioDataset(chiens_train, chats_train, train_transform)\n",
        "valid_data = RadioDataset(chiens_test, chats_test, validation_transform)\n",
        "train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oens5f6tXigh"
      },
      "source": [
        "#### Lancement du training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZQkO5ZXreC"
      },
      "outputs": [],
      "source": [
        "# la fonction fit déroule le training \n",
        "def fit(model,dataloader,data,optimizer,criterion):\n",
        "    print('-------------Training---------------')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    counter=0\n",
        "    \n",
        "    # num of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    for i,data in tqdm(enumerate(dataloader),total = num_batches):\n",
        "        counter+=1\n",
        "        image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image)\n",
        "        outputs = outputs.squeeze(1)\n",
        "        loss = criterion(outputs,mask)\n",
        "        train_running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss = train_running_loss/counter\n",
        "    return train_loss\n",
        "\n",
        "# la fonction validate évalue le modèle à la fin de chaque epoch\n",
        "def validate(model,dataloader,data,criterion):\n",
        "    print(\"\\n--------Validating---------\\n\")\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n",
        "            counter+=1\n",
        "            image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "            outputs = model(image)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs,mask)\n",
        "            valid_running_loss += loss.item()\n",
        "    valid_loss = valid_running_loss/counter\n",
        "    return valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjHcQ48LZsYd"
      },
      "outputs": [],
      "source": [
        "# Execution du training et sauvegarde du meilleur modèle et de la courbe des Loss\n",
        "path_save = \"drive/MyDrive/model.pth\"\n",
        "\n",
        "lowest_val_loss=10.\n",
        "train_loss = []\n",
        "val_loss =[]\n",
        "model = UNet(NB_CHANNELS, NB_CLASSES).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss() # mais la BCEWithLogitsLoss est plus performante\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
        "    train_epoch_loss = fit(model, train_dataloader, train_data, optimizer, criterion)\n",
        "    val_epoch_loss = validate(model, valid_dataloader, valid_data, criterion)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f}')\n",
        "    if val_epoch_loss < lowest_val_loss:  # On ne sauvegarde le modèle que si on réduit la validation Loss  \n",
        "        lowest_val_loss= val_epoch_loss\n",
        "        torch.save({\n",
        "        'epoch': EPOCHS,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': criterion,\n",
        "          }, path_save)\n",
        "\n",
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color=\"orange\", label='train loss')\n",
        "plt.plot(val_loss, color=\"red\", label='validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"drive/MyDrive/loss.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoLJubcXLlgQ"
      },
      "source": [
        "#### Test du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_qNNwLnf1ky"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = path_save\n",
        "\n",
        "radio2 = valid_data[11][\"image\"]\n",
        "mask2 = valid_data[11][\"mask\"]\n",
        "\n",
        "model = UNet(1, 4)\n",
        "model.load_state_dict(torch.load(MODEL_PATH)[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "\n",
        "output = model(radio2.unsqueeze(1).to(DEVICE))\n",
        "\n",
        "output2 = output[0].cpu().data.numpy()\n",
        "index = output2.argmax(axis=0)\n",
        "\n",
        "index = index*150\n",
        "fig = plt.figure()\n",
        "plt.imshow(index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(mask2.to('cpu'))"
      ],
      "metadata": {
        "id": "dCbxatE99Sga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(radio2[0])"
      ],
      "metadata": {
        "id": "wXvzyvrv9SqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0Zb4ECnTijSN",
        "irBzptlBK18c",
        "1xF_zEzHK6uS",
        "RyjPW8B2LAyh",
        "E2bECYijM3b-",
        "etGkxf6aY-nf"
      ],
      "name": "Training_avec_Resizing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}