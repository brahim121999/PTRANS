{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuNJWHqZV3t"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Notebook pour le training de modèle pour le PTRANS**\n",
        "\n",
        "☕*Created by :*\n",
        "\n",
        "*   Mattéo Boursault\n",
        "*   Ibrahim Braham\n",
        "*   Adam Creusevault\n",
        "\n",
        "Dernière modification : *25/03/2022*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faOh2PruarEc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "- Complétez le chemin des 'Verite_terrain'\n",
        "- Modifier les paramètres dans la section Configuration au besoin\n",
        "- Executer toutes les cellules du Notebook\n",
        "- Have fun\n",
        "'''\n",
        "VERITE_PATH = 'drive/MyDrive/PTRANS/ptrans-main/Dev/data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zb4ECnTijSN"
      },
      "source": [
        "#### Import + Connexion au Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld2XJNfdefn2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCQ9g--jgH-5"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!CUDA_LAUNCH_BLOCKING=1 # plus utile de croiser les doigts mais bon "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PELvrwSvempV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import copy\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irBzptlBK18c"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY5OHw3RgIva"
      },
      "outputs": [],
      "source": [
        "class RadioDataset(Dataset):\n",
        "    def __init__(self, inputs_chiens, inputs_chats, transform=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # Chemins des dossiers de la vérité terrain\n",
        "        ROOT = VERITE_PATH + 'Verite_terrain/'\n",
        "        DOG_PATH = 'Chiens/'\n",
        "        CAT_PATH = 'Chats/'\n",
        "        RADIOS = 'Radios/'\n",
        "        HEART_MASKS = 'Coeur/'\n",
        "        VTB_MASKS = 'Vertebres/'\n",
        "        PROCESS_MASKS = 'Process_epineux/'\n",
        "\n",
        "        # Récupération et stockage des chemins des fichiers\n",
        "        dog_radios = self.generate_chemin(ROOT + DOG_PATH + RADIOS, inputs_chiens)\n",
        "        cat_radios = self.generate_chemin(ROOT + CAT_PATH + RADIOS, inputs_chats)\n",
        "\n",
        "        dog_hearts = self.generate_chemin(ROOT + DOG_PATH + HEART_MASKS, inputs_chiens, True)\n",
        "        cat_hearts = self.generate_chemin(ROOT + CAT_PATH + HEART_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_vtb = self.generate_chemin(ROOT + DOG_PATH + VTB_MASKS, inputs_chiens, True)\n",
        "        cat_vtb = self.generate_chemin(ROOT + CAT_PATH + VTB_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_prc = self.generate_chemin(ROOT + DOG_PATH + PROCESS_MASKS, inputs_chiens, True)\n",
        "        cat_prc = self.generate_chemin(ROOT + CAT_PATH + PROCESS_MASKS, inputs_chats, True)\n",
        "\n",
        "        self.radios_arr = dog_radios + cat_radios\n",
        "        self.heart_arr = dog_hearts + cat_hearts\n",
        "        self.vtb_arr = dog_vtb + cat_vtb\n",
        "        self.process_arr = dog_prc + cat_prc\n",
        "\n",
        "        self.data_len = len(self.radios_arr)\n",
        "\n",
        "        print(\"GROUND_TRUTH FOUND : \", self.data_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fonction obligatoire utilisée par le loader lors de l'itération du dataset\n",
        "        Récupération du chemin et chargement de la vérité terrain puis labellisation\n",
        "        \"\"\"\n",
        "\n",
        "        radio = np.array(Image.open(self.radios_arr[idx]))\n",
        "\n",
        "        heart_mask = self.grayscale(self.heart_arr[idx])\n",
        "        vtb_mask = self.grayscale(self.vtb_arr[idx])\n",
        "        process_mask = self.grayscale(self.process_arr[idx])\n",
        "\n",
        "        heart_mask[heart_mask != 0] = 1\n",
        "        vtb_mask[vtb_mask != 0] = 2\n",
        "        process_mask[process_mask != 0] = 3\n",
        "        #masks = heart_mask + vtb_mask + process_mask\n",
        "\n",
        "        \n",
        "        masks = np.zeros_like(heart_mask)\n",
        "        masks[heart_mask == 1] = 1\n",
        "        masks[vtb_mask == 2] = 2\n",
        "        masks[process_mask == 3] = 3\n",
        "        \n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=radio,mask=masks)\n",
        "            radio = augmentations['image']\n",
        "            masks = augmentations['mask']\n",
        "\n",
        "        grayscale_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                                  transforms.ToTensor()])\n",
        "        radio = Image.fromarray(radio)\n",
        "        radio = grayscale_transform(radio)\n",
        "\n",
        "        \"\"\"\n",
        "        background = np.zeros_like(masks) \n",
        "        heart_mask = np.zeros_like(masks)\n",
        "        vtb_mask = np.zeros_like(masks)\n",
        "        process_mask = np.zeros_like(masks)\n",
        "\n",
        "        background[masks == 0] = 1\n",
        "        heart_mask[masks == 1] = 1\n",
        "        vtb_mask[masks == 2] = 1\n",
        "        process_mask[masks == 3] = 1\n",
        "        \n",
        "        #masks = torch.tensor([heart_mask, vtb_mask, process_mask, background])\n",
        "        masks = np.array([heart_mask, vtb_mask, process_mask, background])\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Utiliser from_numpy plutôt que toTensor permet de ne pas normaliser les\n",
        "        données afin de respecter la labellisation des classes par 0, 1, 2 et 3\n",
        "        \"\"\"\n",
        "        return {\"image\": radio, \"mask\": torch.from_numpy(masks).to(DEVICE)}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Méthode nécessaire au loader qui retourne le\n",
        "        nombre de données dans le dataset\n",
        "        \"\"\"\n",
        "        return self.data_len\n",
        "\n",
        "\n",
        "    def grayscale(self, path):\n",
        "        image = Image.open(path)\n",
        "        image.load()\n",
        "        background = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
        "        background.paste(image, mask=image.split()[3])\n",
        "        image = background\n",
        "        image = image.convert('L')\n",
        "        return np.array(image)\n",
        "\n",
        "    def generate_chemin(self, path, tableau, jpgTopng=False):\n",
        "        tab = []\n",
        "        if (jpgTopng):\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i][0:-3] + 'png')\n",
        "        else:\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i])\n",
        "        return tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF_zEzHK6uS"
      },
      "source": [
        "#### Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KI1mPoWg0p9I"
      },
      "outputs": [],
      "source": [
        "# modele UNet :\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Transposed convolution then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels , out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward_padding(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = nn.functionnal.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "    def forward(self, target_tensor, contracting_tensor):\n",
        "        target_tensor = self.up(target_tensor)\n",
        "        target_height = target_tensor.size()[2]\n",
        "        target_width = target_tensor.size()[3]\n",
        "        crop = transforms.CenterCrop((target_height, target_width))\n",
        "\n",
        "        contracting_tensor = crop(contracting_tensor)\n",
        "        new_tensor = torch.cat((target_tensor, contracting_tensor), 1)\n",
        "\n",
        "        return self.conv(new_tensor)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjPW8B2LAyh"
      },
      "source": [
        "#### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q5oDHw-0Fbhu"
      },
      "outputs": [],
      "source": [
        "MAX_SIZE = 560\n",
        "MODELE = UNet\n",
        "PATH_SAVE = 'drive/MyDrive/'\n",
        "NB_CLASSES = 4\n",
        "NB_CHANNELS = 1\n",
        "LEARNING_RATE = 0.001\n",
        "TRAIN_SIZE=0.9\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 80\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bECYijM3b-"
      },
      "source": [
        "#### Data Augmentations (with Albumentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jSJz3AgW9UWf"
      },
      "outputs": [],
      "source": [
        "# les transformations à appliquer à partir de la bibliothèque Albumentations qui vont servir à resize les radios et les masques en dimension unique 572 x 572\n",
        "# la réduction des dimensions va permettre que le training se déroule sans plantage de la GPU dû à un débordement de mémoire  \n",
        "# et aussi permettre la data augmentation en appliquant un nombre important d'epochs car les transformations sont aléatoires.\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    #A.Rotate(limit=20,p=0.4),\n",
        "    #A.HorizontalFlip(p=0.4),\n",
        "    #A.VerticalFlip(p=0.4),\n",
        "    #A.Transpose(p=0.4),\n",
        "    #A.GridDistortion(p=0.2),\n",
        "], additional_targets={'mask': 'image'})\n",
        "\n",
        "validation_transform = A.Compose([\n",
        "], additional_targets={'mask': 'image'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etGkxf6aY-nf"
      },
      "source": [
        "#### Création des Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAPLPp8WNue4"
      },
      "outputs": [],
      "source": [
        "# on a éclaté les data (radios et masques) en data train (90%) et data validation (10%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inputs_chiens = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chiens/Radios')\n",
        "inputs_chats = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chats/Radios')\n",
        "\n",
        "chiens_train, chiens_test = train_test_split(inputs_chiens, train_size=TRAIN_SIZE)\n",
        "chats_train, chats_test = train_test_split(inputs_chats, train_size=TRAIN_SIZE)\n",
        "\n",
        "# création des datasets pour le train et pour la validation\n",
        "# création des dataloaders pour le train et pour la validation\n",
        "\n",
        "train_data = RadioDataset(chiens_train, chats_train, train_transform)\n",
        "valid_data = RadioDataset(chiens_test, chats_test, validation_transform)\n",
        "train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oens5f6tXigh"
      },
      "source": [
        "#### Lancement du training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZQkO5ZXreC"
      },
      "outputs": [],
      "source": [
        "def splitSize(img_height, img_width, max_size):\n",
        "  tab = []\n",
        "  nb_batchs_height = img_height//max_size\n",
        "  nb_batchs_width = img_width//max_size\n",
        "\n",
        "  rest_height = img_height % max_size\n",
        "  rest_width = img_width % max_size\n",
        "\n",
        "  for i in range(nb_batchs_height):\n",
        "    for j in range(nb_batchs_width):\n",
        "      tab.append([i*max_size, (i+1)*max_size, j*max_size, (j+1)*max_size])\n",
        "\n",
        "  # on rajoute les bords\n",
        "  if rest_width != 0:\n",
        "    for i in range(nb_batchs_height):\n",
        "      tab.append([i*max_size, (i+1)*max_size, img_width-max_size, img_width])\n",
        "\n",
        "  if rest_height != 0:\n",
        "    for j in range(nb_batchs_width):\n",
        "        tab.append([img_height-max_size, img_height, j*max_size, (j+1)*max_size])\n",
        "  \n",
        "  # on ajoute l'angle\n",
        "  if rest_height != 0 and rest_width != 0:\n",
        "    tab.append([img_height-max_size, img_height, img_width-max_size, img_width])\n",
        "  return tab\n",
        "\n",
        "# la fonction fit déroule le training \n",
        "def fit(model,dataloader,data,optimizer,criterion):\n",
        "    print('-------------Training---------------')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    counter=0\n",
        "    \n",
        "    # num of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    for i,data in tqdm(enumerate(dataloader),total = num_batches):\n",
        "        counter+=1\n",
        "        image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # pour chaque morceaux d'image\n",
        "        tab = splitSize(image.shape[2], image.shape[3], MAX_SIZE)\n",
        "        batch_loss = 0.0\n",
        "        for i in tab:\n",
        "          outputs = model(image[:, :, i[0]:i[1], i[2]:i[3]])\n",
        "          outputs = outputs.squeeze(1)\n",
        "          loss = criterion(outputs,mask[:, i[0]:i[1], i[2]:i[3]])\n",
        "          batch_loss += loss.item()\n",
        "          \n",
        "        train_running_loss += batch_loss/len(tab)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_running_loss/counter\n",
        "    return train_loss\n",
        "\n",
        "# la fonction validate évalue le modèle à la fin de chaque epoch\n",
        "def validate(model,dataloader,data,criterion):\n",
        "    print(\"\\n--------Validating---------\\n\")\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n",
        "            counter+=1\n",
        "            image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "\n",
        "            # pour chaque morceaux d'image\n",
        "            tab = splitSize(image.shape[2], image.shape[3], MAX_SIZE)\n",
        "            batch_loss = 0.0\n",
        "            for i in tab:\n",
        "              outputs = model(image[:, :, i[0]:i[1], i[2]:i[3]])\n",
        "              outputs = outputs.squeeze(1)\n",
        "              loss = criterion(outputs,mask[:, i[0]:i[1], i[2]:i[3]])\n",
        "              batch_loss += loss.item()\n",
        "\n",
        "            valid_running_loss += batch_loss/len(tab)\n",
        "            \n",
        "    valid_loss = valid_running_loss/counter\n",
        "    return valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tjHcQ48LZsYd"
      },
      "outputs": [],
      "source": [
        "# Execution du training et sauvegarde du meilleur modèle et de la courbe des Loss\n",
        "\n",
        "path_save = \"drive/MyDrive/model.pth\"\n",
        "lowest_val_loss = 10.\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "##### Pour repartir d'un modèle déjà entrainé #####\n",
        "# model = UNet(NB_CHANNELS, NB_CLASSES).to(DEVICE)\n",
        "model_path = PATH_SAVE + \"U-net_loss0.2239.pth\"\n",
        "model = UNet(1, 4)\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n",
        "model.to(DEVICE)\n",
        "lowest_val_loss = 0.2239\n",
        "###################################################\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss() # mais la BCEWithLogitsLoss est plus performante\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
        "    train_epoch_loss = fit(model, train_dataloader, train_data, optimizer, criterion)\n",
        "    val_epoch_loss = validate(model, valid_dataloader, valid_data, criterion)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f}')\n",
        "    if val_epoch_loss < lowest_val_loss:  # On ne sauvegarde le modèle que si on réduit la validation Loss  \n",
        "        lowest_val_loss= val_epoch_loss\n",
        "        torch.save({\n",
        "        'epoch': EPOCHS,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': criterion,\n",
        "          }, path_save)\n",
        "\n",
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color=\"orange\", label='train loss')\n",
        "plt.plot(val_loss, color=\"red\", label='validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"drive/MyDrive/loss.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoLJubcXLlgQ"
      },
      "source": [
        "#### Test du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_qNNwLnf1ky"
      },
      "outputs": [],
      "source": [
        "model_path = PATH_SAVE + \"U-net_40ep_loss0.202.pth\"\n",
        "\n",
        "radio2 = valid_data[1][\"image\"].to(DEVICE)\n",
        "\n",
        "model = UNet(1, 4)\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "\n",
        "output = model(radio2.unsqueeze(1).to(DEVICE))\n",
        "\n",
        "output2 = output[0].cpu().data.numpy()\n",
        "index = output2.argmax(axis=0)\n",
        "\n",
        "index = index*100\n",
        "plt.imshow(index)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "irBzptlBK18c",
        "1xF_zEzHK6uS",
        "RyjPW8B2LAyh",
        "etGkxf6aY-nf"
      ],
      "name": "Training_avec_Splitting",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}