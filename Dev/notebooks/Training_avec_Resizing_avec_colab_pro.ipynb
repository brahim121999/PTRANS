{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuNJWHqZV3t"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Notebook pour le training de modèle pour le PTRANS**\n",
        "\n",
        "☕*Created by :*\n",
        "\n",
        "*   Mattéo Boursault\n",
        "*   Ibrahim Braham\n",
        "*   Adam Creusevault\n",
        "\n",
        "Dernière modification : *06/05/2022*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qapxr_M6lAfs"
      },
      "source": [
        "**pour laisser tourner sur une longue durée**\n",
        "\n",
        "```\n",
        "var startClickConnect = function startClickConnect(){\n",
        "    var clickConnect = function clickConnect(){\n",
        "        console.log(\"Connnect Clicked - Start\");\n",
        "        document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "        console.log(\"Connnect Clicked - End\"); \n",
        "    };\n",
        "\n",
        "    var intervalId = setInterval(clickConnect, 180000);\n",
        "\n",
        "    var stopClickConnectHandler = function stopClickConnect() {\n",
        "        console.log(\"Connnect Clicked Stopped - Start\");\n",
        "        clearInterval(intervalId);\n",
        "        console.log(\"Connnect Clicked Stopped - End\");\n",
        "    };\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faOh2PruarEc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "- Complétez le chemin des 'Verite_terrain'\n",
        "- Modifiez les paramètres dans la section Configuration au besoin\n",
        "- Executez toutes les cellules du Notebook\n",
        "- Have fun\n",
        "'''\n",
        "VERITE_PATH = 'drive/MyDrive/PTRANS/ptrans-main/Dev/data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zb4ECnTijSN"
      },
      "source": [
        "#### Import + Connexion au Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld2XJNfdefn2",
        "outputId": "ce8312a1-b22b-481d-9bff-37a03d4e2e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCQ9g--jgH-5",
        "outputId": "437e899c-615d-4a84-ecc1-8adb91cac15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.2.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=4adb53f10ff374698ef3d9ae7376d535a63a3cce89dc84044a734061c972f914\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.6-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.6\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!pip install torchinfo\n",
        "!CUDA_LAUNCH_BLOCKING=1 # pour debug CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PELvrwSvempV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "from pynvml import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import copy\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC8CsW05Fbw3",
        "outputId": "e70dd6fd-d28c-49e0-9e59-d470332c6bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total    : 17071734784\n",
            "free     : 17071734784\n",
            "used     : 0\n"
          ]
        }
      ],
      "source": [
        "# pour afficher la mémoire libre/utilisée de la GPU \n",
        "# pour lancer le training avec une taille de radios importante, il faut vérifier que toute la mémoire de la GPU est libre sinon redémarrer l'environnement d'exécution.\n",
        "nvmlInit()\n",
        "h = nvmlDeviceGetHandleByIndex(0)\n",
        "info = nvmlDeviceGetMemoryInfo(h)\n",
        "print(f'total    : {info.total}') \n",
        "print(f'free     : {info.free}')\n",
        "print(f'used     : {info.used}') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irBzptlBK18c"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY5OHw3RgIva"
      },
      "outputs": [],
      "source": [
        "class RadioDataset(Dataset):\n",
        "    def __init__(self, inputs_chiens, inputs_chats, transform=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # Chemins des dossiers de la vérité terrain\n",
        "        ROOT = VERITE_PATH + 'Verite_terrain/'\n",
        "        DOG_PATH = 'Chiens/'\n",
        "        CAT_PATH = 'Chats/'\n",
        "        RADIOS = 'Radios/'\n",
        "        HEART_MASKS = 'Coeur/'\n",
        "        VTB_MASKS = 'Vertebres/'\n",
        "        PROCESS_MASKS = 'Process_epineux/'\n",
        "\n",
        "        # Récupération et stockage des chemins des fichiers\n",
        "        dog_radios = self.generate_chemin(ROOT + DOG_PATH + RADIOS, inputs_chiens)\n",
        "        cat_radios = self.generate_chemin(ROOT + CAT_PATH + RADIOS, inputs_chats)\n",
        "\n",
        "        dog_hearts = self.generate_chemin(ROOT + DOG_PATH + HEART_MASKS, inputs_chiens, True)\n",
        "        cat_hearts = self.generate_chemin(ROOT + CAT_PATH + HEART_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_vtb = self.generate_chemin(ROOT + DOG_PATH + VTB_MASKS, inputs_chiens, True)\n",
        "        cat_vtb = self.generate_chemin(ROOT + CAT_PATH + VTB_MASKS, inputs_chats, True)\n",
        "\n",
        "        dog_prc = self.generate_chemin(ROOT + DOG_PATH + PROCESS_MASKS, inputs_chiens, True)\n",
        "        cat_prc = self.generate_chemin(ROOT + CAT_PATH + PROCESS_MASKS, inputs_chats, True)\n",
        "\n",
        "        self.radios_arr = dog_radios + cat_radios\n",
        "        self.heart_arr = dog_hearts + cat_hearts\n",
        "        self.vtb_arr = dog_vtb + cat_vtb\n",
        "        self.process_arr = dog_prc + cat_prc\n",
        "\n",
        "        self.data_len = len(self.radios_arr)\n",
        "\n",
        "        print(\"GROUND_TRUTH FOUND : \", self.data_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fonction obligatoire utilisée par le loader lors de l'itération du dataset\n",
        "        Récupération du chemin et chargement de la vérité terrain puis labellisation\n",
        "        \"\"\"\n",
        "\n",
        "        radio = np.array(Image.open(self.radios_arr[idx]))\n",
        "\n",
        "        heart_mask = self.grayscale(self.heart_arr[idx])\n",
        "        vtb_mask = self.grayscale(self.vtb_arr[idx])\n",
        "        process_mask = self.grayscale(self.process_arr[idx])\n",
        "\n",
        "        heart_mask[heart_mask != 0] = 1\n",
        "        vtb_mask[vtb_mask != 0] = 2\n",
        "        process_mask[process_mask != 0] = 3\n",
        "        #masks = heart_mask + vtb_mask + process_mask\n",
        "\n",
        "        \n",
        "        masks = np.zeros_like(heart_mask)\n",
        "        masks[heart_mask == 1] = 1\n",
        "        masks[vtb_mask == 2] = 2\n",
        "        masks[process_mask == 3] = 3\n",
        "        \n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=radio,mask=masks)\n",
        "            radio = augmentations['image']\n",
        "            masks = augmentations['mask']\n",
        "\n",
        "        grayscale_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                                  transforms.ToTensor()])\n",
        "        radio = Image.fromarray(radio)\n",
        "        radio = grayscale_transform(radio)\n",
        "\n",
        "        \"\"\"\n",
        "        background = np.zeros_like(masks) \n",
        "        heart_mask = np.zeros_like(masks)\n",
        "        vtb_mask = np.zeros_like(masks)\n",
        "        process_mask = np.zeros_like(masks)\n",
        "\n",
        "        background[masks == 0] = 1\n",
        "        heart_mask[masks == 1] = 1\n",
        "        vtb_mask[masks == 2] = 1\n",
        "        process_mask[masks == 3] = 1\n",
        "        \n",
        "        #masks = torch.tensor([heart_mask, vtb_mask, process_mask, background])\n",
        "        masks = np.array([heart_mask, vtb_mask, process_mask, background])\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Utiliser from_numpy plutôt que toTensor permet de ne pas normaliser les\n",
        "        données afin de respecter la labellisation des classes par 0, 1, 2 et 3\n",
        "        \"\"\"\n",
        "        return {\"image\": radio, \"mask\": torch.from_numpy(masks).to(DEVICE)}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Méthode nécessaire au loader qui retourne le\n",
        "        nombre de données dans le dataset\n",
        "        \"\"\"\n",
        "        return self.data_len\n",
        "\n",
        "\n",
        "    def grayscale(self, path):\n",
        "        image = Image.open(path)\n",
        "        image.load()\n",
        "        background = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
        "        background.paste(image, mask=image.split()[3])\n",
        "        image = background\n",
        "        image = image.convert('L')\n",
        "        return np.array(image)\n",
        "\n",
        "    def generate_chemin(self, path, tableau, jpgTopng=False):\n",
        "        tab = []\n",
        "        if (jpgTopng):\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i][0:-3] + 'png')\n",
        "        else:\n",
        "          for i in range(len(tableau)):\n",
        "            tab.append(path + tableau[i])\n",
        "        return tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF_zEzHK6uS"
      },
      "source": [
        "#### Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI1mPoWg0p9I"
      },
      "outputs": [],
      "source": [
        "# modele UNet :\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Transposed convolution then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels , out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward_padding(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = nn.functionnal.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "    def forward(self, target_tensor, contracting_tensor):\n",
        "        target_tensor = self.up(target_tensor)\n",
        "        target_height = target_tensor.size()[2]\n",
        "        target_width = target_tensor.size()[3]\n",
        "        crop = transforms.CenterCrop((target_height, target_width))\n",
        "\n",
        "        contracting_tensor = crop(contracting_tensor)\n",
        "        new_tensor = torch.cat((target_tensor, contracting_tensor), 1)\n",
        "\n",
        "        return self.conv(new_tensor)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB3fDmNhTeYJ"
      },
      "outputs": [],
      "source": [
        "# nouveau modèle  avec 124 millions de paramètres\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def double_conv(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return conv.to(DEVICE)\n",
        "\n",
        "# def addPadding(srcShapeTensor, tensor_whose_shape_isTobechanged):\n",
        "\n",
        "#     if(srcShapeTensor.shape != tensor_whose_shape_isTobechanged.shape):\n",
        "#         target = torch.zeros(srcShapeTensor.shape)\n",
        "#         target[:, :, :tensor_whose_shape_isTobechanged.shape[2],\n",
        "#                :tensor_whose_shape_isTobechanged.shape[3]] = tensor_whose_shape_isTobechanged\n",
        "#         return target.to(DEVICE)\n",
        "#     return tensor_whose_shape_isTobechanged.to(DEVICE)\n",
        "\n",
        "class UNet2(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet2, self).__init__()\n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.down_conv_1 = double_conv(3, 64)\n",
        "        self.down_conv_2 = double_conv(64, 128)\n",
        "        self.down_conv_3 = double_conv(128, 256)\n",
        "        self.down_conv_4 = double_conv(256, 512)\n",
        "        self.down_conv_5 = double_conv(512, 1024)\n",
        "\n",
        "# new by Braham\n",
        "        self.down_conv_6 = double_conv(1024, 2048)\n",
        "        \n",
        "        self.up_trans_0 = nn.ConvTranspose2d(\n",
        "            in_channels=2048,\n",
        "            out_channels=1024,\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.up_conv_0 = double_conv(2048, 1024)\n",
        "# fin de new\n",
        "\n",
        "        self.up_trans_1 = nn.ConvTranspose2d(\n",
        "            in_channels=1024,\n",
        "            out_channels=512,\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.up_conv_1 = double_conv(1024, 512)\n",
        "\n",
        "        self.up_trans_2 = nn.ConvTranspose2d(\n",
        "            in_channels=512,\n",
        "            out_channels=256,\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.up_conv_2 = double_conv(512, 256)\n",
        "\n",
        "        self.up_trans_3 = nn.ConvTranspose2d(\n",
        "            in_channels=256,\n",
        "            out_channels=128,\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.up_conv_3 = double_conv(256, 128)\n",
        "\n",
        "        self.up_trans_4 = nn.ConvTranspose2d(\n",
        "            in_channels=128,\n",
        "            out_channels=64,\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.up_conv_4 = double_conv(128, 64)\n",
        "\n",
        "        self.out = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=n_classes,\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        x1 = self.down_conv_1(image)\n",
        "        x2 = self.max_pool_2x2(x1)\n",
        "        x3 = self.down_conv_2(x2)\n",
        "        x4 = self.max_pool_2x2(x3)\n",
        "        x5 = self.down_conv_3(x4)\n",
        "        x6 = self.max_pool_2x2(x5)\n",
        "        x7 = self.down_conv_4(x6)        \n",
        "        x8 = self.max_pool_2x2(x7)\n",
        "        x9 = self.down_conv_5(x8)\n",
        "\n",
        "# new by Braham   \n",
        "        x10 = self.max_pool_2x2(x9)\n",
        "        x11 = self.down_conv_6(x10)  \n",
        "        x = self.up_trans_0(x11)\n",
        "        x = self.up_conv_0(torch.cat([x9, x], 1))        \n",
        "# fin de new\n",
        "        \n",
        "        x = self.up_trans_1(x9)\n",
        "        x = self.up_conv_1(torch.cat([x7, x], 1))        \n",
        "        x = self.up_trans_2(x)\n",
        "        x = self.up_conv_2(torch.cat([x5, x], 1))   \n",
        "        x = self.up_trans_3(x)\n",
        "        x = self.up_conv_3(torch.cat([x3, x], 1))\n",
        "        x = self.up_trans_4(x)\n",
        "        x = self.up_conv_4(torch.cat([x1, x], 1))\n",
        "        x = self.out(x)\n",
        "        return x.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjPW8B2LAyh"
      },
      "source": [
        "#### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oDHw-0Fbhu"
      },
      "outputs": [],
      "source": [
        "MODELE = UNet                 # modèle utilisé, voir dans la partie Modèle du notebook la liste des modèles disponibles\n",
        "TRAINED_MODEL = None          # definit le nom du modèle que vous souhaitez réentrainer, si ce n'est pas cas None\n",
        "PATH_SAVE = 'drive/MyDrive/'  # definit le chemin du dossier qui contient le modèle que vous souhaitez réentrainer\n",
        "NB_CLASSES = 4                # 4 pour : coeur, vertebres, process_epineux, fond\n",
        "NB_CHANNELS = 1               # un vecteur en sortie (la prédiction du modèle)\n",
        "LEARNING_RATE = 0.001         # le taux d'apprentissage\n",
        "TRAIN_SIZE = 0.8              # le % de segmentations allouées au jeu d'entrainement (1-TRAIN_SIZE pour la taille du jeu de validation)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # pour utiliser CUDA si nécessaire, ne pas toucher\n",
        "BATCH_SIZE = 1                # la taille de votre batch\n",
        "EPOCHS = 1000                 # le nombre de fois ou vous souhaitez itérer sur l'ensemble du jeu de donnée pour entrainer votre modèle\n",
        "SIZE = 1472                   # la taille souhaité des radio en entrée du modèle (max 572 sans colab pro, 1520 avec), doit être un multiple de 16 (prérequis du modèle UNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bECYijM3b-"
      },
      "source": [
        "#### Data Augmentations (with Albumentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSJz3AgW9UWf"
      },
      "outputs": [],
      "source": [
        "# les transformations à appliquer à partir de la bibliothèque Albumentations qui vont servir à resize les radios et les masques en dimension unique\n",
        "# la réduction des dimensions va permettre que le training se déroule sans plantage de la GPU dû à un débordement de mémoire  \n",
        "# et aussi permettre la data augmentation en appliquant un nombre important d'epochs car les transformations sont aléatoires.\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Rotate(limit=10,p=0.3),\n",
        "    A.HorizontalFlip(p=0.3),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Transpose(p=0.3),\n",
        "    #A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, brightness_by_max=True, always_apply=False, p=0.3),\n",
        "    A.GridDistortion(p=0.2),\n",
        "    A.LongestMaxSize(SIZE),\n",
        "    A.PadIfNeeded(min_height=SIZE, min_width=SIZE, border_mode=cv2.BORDER_CONSTANT),\n",
        "], additional_targets={'mask': 'image'})\n",
        "\n",
        "validation_transform = A.Compose([\n",
        "    A.LongestMaxSize(SIZE),\n",
        "    A.PadIfNeeded(min_height=SIZE, min_width=SIZE, border_mode=cv2.BORDER_CONSTANT),\n",
        "], additional_targets={'mask': 'image'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etGkxf6aY-nf"
      },
      "source": [
        "#### Création des Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAPLPp8WNue4",
        "outputId": "c2224173-5ac0-4399-96de-0e17e31b0adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GROUND_TRUTH FOUND :  205\n",
            "GROUND_TRUTH FOUND :  52\n"
          ]
        }
      ],
      "source": [
        "# on a éclaté les data (radios et masques) en data train et data validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inputs_chiens = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chiens/Radios')\n",
        "inputs_chats = os.listdir('drive/MyDrive/PTRANS/ptrans-main/Dev/data/Verite_terrain/Chats/Radios')\n",
        "\n",
        "chiens_train, chiens_test = train_test_split(inputs_chiens, train_size=TRAIN_SIZE, random_state = 79)\n",
        "chats_train, chats_test = train_test_split(inputs_chats, train_size=TRAIN_SIZE, random_state = 79)\n",
        "\n",
        "# création des datasets pour le train et pour la validation\n",
        "# création des dataloaders pour le train et pour la validation\n",
        "\n",
        "train_data = RadioDataset(chiens_train, chats_train, train_transform)\n",
        "valid_data = RadioDataset(chiens_test, chats_test, validation_transform)\n",
        "train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oens5f6tXigh"
      },
      "source": [
        "#### Lancement du training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZQkO5ZXreC"
      },
      "outputs": [],
      "source": [
        "# la fonction fit déroule le training \n",
        "def fit(model,dataloader,data,optimizer,criterion):\n",
        "    print('-------------Training---------------')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    counter = 0\n",
        "    \n",
        "    # num of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    for i,data in tqdm(enumerate(dataloader),total = num_batches):\n",
        "        counter+=1\n",
        "        image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image)\n",
        "        outputs = outputs.squeeze(1)\n",
        "        loss = criterion(outputs,mask)\n",
        "        train_running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss = train_running_loss/counter\n",
        "    return train_loss\n",
        "\n",
        "# la fonction validate évalue le modèle à la fin de chaque epoch\n",
        "def validate(model,dataloader,data,criterion):\n",
        "    print(\"\\n--------Validating---------\\n\")\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "    counter = 0\n",
        "    # number of batches\n",
        "    num_batches = int(len(data)/dataloader.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n",
        "            counter+=1\n",
        "            image,mask = data[\"image\"].to(DEVICE), data[\"mask\"].type('torch.LongTensor').to(DEVICE)\n",
        "            outputs = model(image)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs,mask)\n",
        "            valid_running_loss += loss.item()\n",
        "    valid_loss = valid_running_loss/counter\n",
        "    return valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tjHcQ48LZsYd"
      },
      "outputs": [],
      "source": [
        "# Execution du training et sauvegarde du meilleur modèle et de la courbe des Loss\n",
        "path_save = \"drive/MyDrive/model.pth\"\n",
        "\n",
        "# Pour repartir d'un modèle déjà entrainé\n",
        "if TRAINED_MODEL != None:\n",
        "  model_path = PATH_SAVE + TRAINED_MODEL\n",
        "  trained_model = torch.load(model_path)\n",
        "  model = UNet(NB_CHANNELS, NB_CLASSES).to(DEVICE)\n",
        "  model.load_state_dict(trained_model[\"model_state_dict\"])\n",
        "\n",
        "  train_array_loss = trained_model[\"train_array_loss\"]\n",
        "  val_array_loss = trained_model[\"val_array_loss\"]\n",
        "  lowest_val_loss = min(val_array_loss)\n",
        "\n",
        "else:\n",
        "  model = UNet(NB_CHANNELS, NB_CLASSES).to(DEVICE)\n",
        "\n",
        "  lowest_val_loss = 10.\n",
        "  train_array_loss = []\n",
        "  val_array_loss = []\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss() # mais la BCEWithLogitsLoss est plus performante\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
        "    train_epoch_loss = fit(model, train_dataloader, train_data, optimizer, criterion)\n",
        "    val_epoch_loss = validate(model, valid_dataloader, valid_data, criterion)\n",
        "    train_array_loss.append(train_epoch_loss)\n",
        "    val_array_loss.append(val_epoch_loss)\n",
        "    print(f'Train Loss: {train_epoch_loss:.4f}')\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f}')\n",
        "    print(f'Actual best val loss: {min(val_array_loss):.4f}')\n",
        "    if val_epoch_loss < lowest_val_loss:  # On ne sauvegarde le modèle que si on réduit la validation Loss  \n",
        "        lowest_val_loss= val_epoch_loss\n",
        "        torch.save({\n",
        "        'epoch': EPOCHS,\n",
        "        'train_array_loss': train_array_loss,\n",
        "        'val_array_loss': val_array_loss,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': criterion,\n",
        "          }, path_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoLJubcXLlgQ"
      },
      "source": [
        "#### Test du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLRlXXbyfD5y"
      },
      "outputs": [],
      "source": [
        "# define the model you want to test\n",
        "MODEL_PATH = PATH_SAVE + 'model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE0XNcRuff1R"
      },
      "outputs": [],
      "source": [
        "model = UNet(1, 4)\n",
        "model_to_test = torch.load(MODEL_PATH)\n",
        "model.load_state_dict(model_to_test[\"model_state_dict\"])\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "\n",
        "train_array_loss = model_to_test[\"train_array_loss\"]\n",
        "val_array_loss = model_to_test[\"val_array_loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-oitmrIZjx7"
      },
      "outputs": [],
      "source": [
        "def display_image_grid(model, dataset, nb_display_img = 10):\n",
        "\n",
        "  cols = 3\n",
        "  rows = nb_display_img\n",
        "  figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 100)) #10*nb_display_img\n",
        "\n",
        "  for i in range(nb_display_img):\n",
        "\n",
        "    radio = valid_data[i][\"image\"]\n",
        "    mask = valid_data[i][\"mask\"].to('cpu')\n",
        "    output = model(radio.unsqueeze(1).to(DEVICE))\n",
        "    output = output[0].cpu().data.numpy()\n",
        "    index = output.argmax(axis=0)\n",
        "    index = index*100\n",
        "\n",
        "    ax[i, 0].imshow(radio[0])\n",
        "    ax[i, 1].imshow(mask)\n",
        "    ax[i, 2].imshow(index)\n",
        "\n",
        "    ax[i, 0].set_title(\"Image: \", fontsize=3*nb_display_img)\n",
        "    ax[i, 1].set_title(\"Truth mask: \", fontsize=3*nb_display_img)\n",
        "    ax[i, 2].set_title(\"predicted mask: \", fontsize=3*nb_display_img)\n",
        "\n",
        "    ax[i, 0].set_axis_off()\n",
        "    ax[i, 1].set_axis_off()\n",
        "    ax[i, 2].set_axis_off()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tdzVwEuaCzG"
      },
      "outputs": [],
      "source": [
        "def display_loss_curve(train_array_loss, val_array_loss):\n",
        "  best_val_loss = min(val_array_loss)\n",
        "  # loss plots\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.plot(train_array_loss, color=\"blue\", label='train loss')\n",
        "  plt.plot(val_array_loss, color=\"red\", label='validation loss')\n",
        "  plt.plot([val_array_loss.index(best_val_loss)], [best_val_loss], 'go', label=\"Best model\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETZS1P48gczO"
      },
      "outputs": [],
      "source": [
        "summary(model, (1, 1, SIZE, SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWJz3vz1gZwS"
      },
      "outputs": [],
      "source": [
        "display_loss_curve(train_array_loss, val_array_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvq4rfWv2il0"
      },
      "outputs": [],
      "source": [
        "len(val_array_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rza9cdBwequN"
      },
      "outputs": [],
      "source": [
        "display_image_grid(model, train_data, nb_display_img = 20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0Zb4ECnTijSN",
        "irBzptlBK18c",
        "1xF_zEzHK6uS",
        "RyjPW8B2LAyh",
        "E2bECYijM3b-"
      ],
      "machine_shape": "hm",
      "name": "Training_avec_Resizing_avec_colab_pro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}